{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a919e2",
   "metadata": {},
   "source": [
    "### 1. Here's a Python program to read a Hadoop configuration file and display the core components of Hadoop:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf4d88e",
   "metadata": {},
   "source": [
    "def display_hadoop_core_components():\n",
    "    with open('hadoop-config.xml', 'r') as file:\n",
    "        config_lines = file.readlines()\n",
    "\n",
    "    core_components = []\n",
    "\n",
    "    for line in config_lines:\n",
    "        if '<name>fs.defaultFS</name>' in line:\n",
    "            core_components.append('HDFS (NameNode and DataNode)')\n",
    "        elif '<name>yarn.resourcemanager.hostname</name>' in line:\n",
    "            core_components.append('YARN (ResourceManager)')\n",
    "        elif '<name>mapreduce.framework.name</name>' in line:\n",
    "            core_components.append('MapReduce (JobTracker and TaskTracker)')\n",
    "\n",
    "    print(\"Core components of Hadoop:\")\n",
    "    for component in core_components:\n",
    "        print(component)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bbf3e5",
   "metadata": {},
   "source": [
    "### 2. Here's an example of a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory using the hdfs library:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bedc20",
   "metadata": {},
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def calculate_hdfs_directory_size(directory):\n",
    "    client = InsecureClient('http://localhost:50070', user='hadoop')\n",
    "\n",
    "    total_size = 0\n",
    "\n",
    "    for root, dirs, files in client.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = root + '/' + file\n",
    "            file_info = client.status(file_path)\n",
    "            file_size = file_info['length']\n",
    "            total_size += file_size\n",
    "\n",
    "    return total_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f8d409",
   "metadata": {},
   "source": [
    "### 3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86738ed",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def extract_top_frequent_words(file_path, top_n):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Preprocess the text by removing non-alphanumeric characters and converting to lowercase\n",
    "    text = re.sub('[^a-zA-Z0-9 ]', '', text.lower())\n",
    "\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Count the frequency of each word using Counter\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    # Get the top N most frequent words\n",
    "    top_frequent_words = word_counts.most_common(top_n)\n",
    "\n",
    "    # Display the top N most frequent words\n",
    "    print(f\"Top {top_n} most frequent words:\")\n",
    "    for word, count in top_frequent_words:\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df63a65",
   "metadata": {},
   "source": [
    "### 4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ea2d3",
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "def check_service_health(service_url):\n",
    "    response = requests.get(service_url)\n",
    "    data = response.json()\n",
    "    if \"healthStatus\" in data:\n",
    "        return data[\"healthStatus\"]\n",
    "    return None\n",
    "\n",
    "#Check the health status of the NameNode\n",
    "namenode_url = \"http://localhost:9870/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo\"\n",
    "namenode_status = check_service_health(namenode_url)\n",
    "if namenode_status:\n",
    "    print(\"NameNode status:\", namenode_status)\n",
    "else:\n",
    "    print(\"Failed to retrieve NameNode status.\")\n",
    "\n",
    "#Check the health status of DataNodes\n",
    "datanodes_url = \"http://localhost:9870/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo\"\n",
    "response = requests.get(datanodes_url)\n",
    "data = response.json()\n",
    "if \"beans\" in data:\n",
    "    datanode_statuses = [node[\"state\"] for node in data[\"beans\"]]\n",
    "    print(\"DataNode statuses:\")\n",
    "    for i, status in enumerate(datanode_statuses):\n",
    "        print(f\"DataNode {i+1}: {status}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve DataNode statuses.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a5ea2a",
   "metadata": {},
   "source": [
    "### 5. Develop a Python program that lists all the files and directories in a specific HDFS path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d13ceca",
   "metadata": {},
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def list_hdfs_path_contents(directory):\n",
    "    client = InsecureClient('http://localhost:9870', user='hadoop')\n",
    "\n",
    "    contents = client.list(directory, status=True)\n",
    "\n",
    "    print(\"Files and directories in\", directory + \":\")\n",
    "    for path, status in contents:\n",
    "        print(\"Path:\", path)\n",
    "        print(\"Type:\", 'File' if status['type'] == 'FILE' else 'Directory')\n",
    "        print(\"Size:\", status['length'] if status['type'] == 'FILE' else '-')\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62d725c",
   "metadata": {},
   "source": [
    "### 6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a79e658",
   "metadata": {},
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def analyze_datanode_storage_utilization():\n",
    "    client = InsecureClient('http://localhost:9870', user='hadoop')\n",
    "\n",
    "    datanodes = client.list('/datanodes', status=True)\n",
    "\n",
    "    if not datanodes:\n",
    "        print(\"No DataNodes found.\")\n",
    "        return\n",
    "\n",
    "    #Sort DataNodes by storage utilization\n",
    "    sorted_datanodes = sorted(datanodes, key=lambda node: node[1]['capacity'], reverse=True)\n",
    "\n",
    "    print(\"DataNode storage utilization analysis:\")\n",
    "    print(\"Highest Storage Capacity:\")\n",
    "    print(\"DataNode:\", sorted_datanodes[0][0])\n",
    "    print(\"Capacity:\", sorted_datanodes[0][1]['capacity'])\n",
    "    print()\n",
    "\n",
    "    print(\"Lowest Storage Capacity:\")\n",
    "    print(\"DataNode:\", sorted_datanodes[-1][0])\n",
    "    print(\"Capacity:\", sorted_datanodes[-1][1]['capacity'])\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2892c7b0",
   "metadata": {},
   "source": [
    "### 7.  Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babeb993",
   "metadata": {},
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(jar_path, input_path, output_path):\n",
    "    # Submit the Hadoop job\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        \"application-id\": \"my_job\",\n",
    "        \"application-name\": \"My Hadoop Job\",\n",
    "        \"am-container-spec\": {\n",
    "            \"commands\": {\n",
    "                \"command\": f\"hadoop jar {jar_path} {input_path} {output_path}\"\n",
    "            }\n",
    "        },\n",
    "        \"container\": {\n",
    "            \"vcores\": 1,\n",
    "            \"memory\": 1024\n",
    "        },\n",
    "        \"application-type\": \"MAPREDUCE\"\n",
    "    }\n",
    "    response = requests.post(\"http://localhost:8088/ws/v1/cluster/apps\", json=data, headers=headers)\n",
    "    if response.status_code == 202:\n",
    "        print(\"Hadoop job submitted successfully.\")\n",
    "    else:\n",
    "        print(\"Failed to submit Hadoop job.\")\n",
    "\n",
    "    # Monitor the progress of the Hadoop job\n",
    "    job_id = response.json()[\"application-id\"]\n",
    "    job_status_url = f\"http://localhost:8088/ws/v1/cluster/apps/{job_id}\"\n",
    "    while True:\n",
    "        response = requests.get(job_status_url)\n",
    "        data = response.json()\n",
    "        state = data[\"app\"][\"state\"]\n",
    "        if state == \"FINISHED\":\n",
    "            print(\"Hadoop job completed.\")\n",
    "            break\n",
    "        elif state == \"FAILED\":\n",
    "            print(\"Hadoop job failed.\")\n",
    "            break\n",
    "        elif state == \"KILLED\":\n",
    "            print(\"Hadoop job killed.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Hadoop job is still running. Progress:\", data[\"app\"][\"progress\"])\n",
    "            time.sleep(5)\n",
    "\n",
    "    # Retrieve the final output of the Hadoop job\n",
    "    final_output_url = f\"http://localhost:8088/ws/v1/cluster/apps/{job_id}/appattempts\"\n",
    "    response = requests.get(final_output_url)\n",
    "    data = response.json()\n",
    "    final_output = data[\"appAttempts\"][\"appAttempt\"][0][\"logsLink\"]\n",
    "\n",
    "    print(\"Final output:\", final_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9058b33",
   "metadata": {},
   "source": [
    "### 8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf3df63",
   "metadata": {},
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(jar_path, input_path, output_path, vcores, memory):\n",
    "    # Submit the Hadoop job\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        \"application-id\": \"my_job\",\n",
    "        \"application-name\": \"My Hadoop Job\",\n",
    "        \"am-container-spec\": {\n",
    "            \"commands\": {\n",
    "                \"command\": f\"hadoop jar {jar_path} {input_path} {output_path}\"\n",
    "            }\n",
    "        },\n",
    "        \"container\": {\n",
    "            \"vcores\": vcores,\n",
    "            \"memory\": memory\n",
    "        },\n",
    "        \"application-type\": \"MAPREDUCE\"\n",
    "    }\n",
    "    response = requests.post(\"http://localhost:8088/ws/v1/cluster/apps\", json=data, headers=headers)\n",
    "    if response.status_code == 202:\n",
    "        print(\"Hadoop job submitted successfully.\")\n",
    "    else:\n",
    "        print(\"Failed to submit Hadoop job.\")\n",
    "\n",
    "    # Get the application ID\n",
    "    job_id = response.json()[\"application-id\"]\n",
    "\n",
    "    # Track resource usage during job execution\n",
    "    while True:\n",
    "        response = requests.get(f\"http://localhost:8088/ws/v1/cluster/apps/{job_id}\")\n",
    "        data = response.json()\n",
    "        state = data[\"app\"][\"state\"]\n",
    "        if state == \"FINISHED\":\n",
    "            print(\"Hadoop job completed.\")\n",
    "            break\n",
    "        elif state == \"FAILED\":\n",
    "            print(\"Hadoop job failed.\")\n",
    "            break\n",
    "        elif state == \"KILLED\":\n",
    "            print(\"Hadoop job killed.\")\n",
    "            break\n",
    "        else:\n",
    "            resources_allocated = data[\"app\"][\"allocatedMB\"]\n",
    "            vcores_allocated = data[\"app\"][\"allocatedVCores\"]\n",
    "            resources_used = data[\"app\"][\"memorySeconds\"]\n",
    "            vcores_used = data[\"app\"][\"vcoreSeconds\"]\n",
    "            print(\"Resource usage:\")\n",
    "            print(\"Allocated Resources (MB):\", resources_allocated)\n",
    "            print(\"Allocated vCores:\", vcores_allocated)\n",
    "            print(\"Used Resources (Memory Seconds):\", resources_used)\n",
    "            print(\"Used vCores Seconds:\", vcores_used)\n",
    "            print()\n",
    "            time.sleep(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd905bb3",
   "metadata": {},
   "source": [
    "### 9.  Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10257ce6",
   "metadata": {},
   "source": [
    "import time\n",
    "import subprocess\n",
    "\n",
    "def run_mapreduce_job(input_path, split_size):\n",
    "    # Set the input split size for the job\n",
    "    hadoop_set_split_size_command = f\"hadoop jar hadoop-streaming.jar -D mapred.max.split.size={split_size} -files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py -input {input_path} -output output\"\n",
    "    \n",
    "    # Run the MapReduce job and measure execution time\n",
    "    start_time = time.time()\n",
    "    subprocess.call(hadoop_set_split_size_command, shell=True)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate the execution time\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    return execution_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
